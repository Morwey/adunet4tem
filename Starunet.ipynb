{"cells":[{"cell_type":"markdown","metadata":{"id":"GRGqhtBa7RWW"},"source":["# Stardist+Unet\n","\n","#### How to Use\n","\n","1. To run demo data, click the three run buttons '‚ñ∑' in order\n","2. To run you own data, allow Google Drive request while runing the first cell, and replace 'Source_image' path with your ***tif*** file path.\n","\n","##### How to replace file path\n","1. Click the folder button 'üóÇÔ∏è' on the left. Click 'drive', and find data at your google drive. Click '‚ãÆ' button on the right of you file. Click 'copy path', and then paste it on the second cell\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kz3PNmGhmixt"},"outputs":[],"source":["%%capture\n","#@markdown ##Initilization and Model Loading(Take up to 3 mins)\n","!pip install gputools > /dev/null 2>&1\n","!pip install stardist > /dev/null 2>&1\n","!pip install -q fpdf2 > /dev/null 2>&1\n","\n","import numpy as np\n","from stardist import fill_label_holes, random_label_cmap, calculate_extents, gputools_available, relabel_image_stardist, random_label_cmap,  relabel_image_stardist, _draw_polygons, export_imagej_rois\n","from stardist.models import Config2D, StarDist2D, StarDistData2D\n","from csbdeep.utils import Path, normalize\n","import skimage\n","import pandas as pd\n","import csv\n","import time\n","from tifffile import imread, imwrite\n","from csbdeep.utils import normalize\n","from stardist.models import StarDist2D\n","from stardist import random_label_cmap\n","import tensorflow as tf\n","\n","from torch.utils.data import DataLoader, random_split\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import torch.nn.utils.prune as prune\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","from tqdm import tqdm\n","import numpy as np\n","\n","from torch.utils.data import Dataset\n","from PIL import Image, ImageOps\n","\n","import requests\n","import zipfile\n","import io\n","from glob import glob\n","\n","from collections import Counter\n","from matplotlib.patches import Patch\n","from scipy import ndimage\n","import cv2\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Unet Structure\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=False, learn = 0.005):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        factor = 2 if bilinear else 1\n","        self.down4 = Down(512, 1024 // factor)\n","        self.up1 = Up(1024, 512 // factor, bilinear)\n","        self.up2 = Up(512, 256 // factor, bilinear)\n","        self.up3 = Up(256, 128 // factor, bilinear)\n","        self.up4 = Up(128, 64, bilinear)\n","        self.outc = OutConv(64, n_classes)\n","\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=learn)\n","\n","        self.history = {\"train_loss\": [], \"val_loss\": []}\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits\n","\n","    def ce_loss(self, pred_mask, target_mask, weighted=False):\n","        target_mask = target_mask.squeeze(1).type(torch.long)\n","        if weighted:\n","            weight = torch.ones(pred_mask.shape[1], device=self.device)\n","            weight[0] = 10\n","            weight[1] = 10\n","            weight[2] = 10\n","            loss = F.cross_entropy(pred_mask, target_mask, weight=weight)\n","        else:\n","            loss = F.cross_entropy(pred_mask, target_mask)\n","        return loss\n","\n","    def focal_loss(self, inputs, targets, alpha=0.25, gamma=2.0, epsilon=1e-9):\n","        inputs = F.softmax(inputs, dim=1)\n","        targets = targets.squeeze(1)  # [batch_size, 1, height, width] to [batch_size, height, width]\n","        targets = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n","\n","        inputs = inputs.clamp(min=epsilon, max=1.0 - epsilon)\n","        pt = torch.where(torch.eq(targets, 1), inputs, 1 - inputs)\n","        loss = -alpha * (1 - pt) ** gamma * torch.log(pt)\n","\n","        return loss.mean()\n","\n","    def dice_loss(self, inputs, targets, smooth=1e-6):\n","        inputs = F.softmax(inputs, dim=1)\n","        targets = targets.squeeze(1)\n","        targets = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n","\n","        intersection = 2.0 * (inputs * targets).sum(dim=(2, 3))\n","        union = inputs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n","        dice_score = (intersection + smooth) / (union + smooth)\n","\n","        return (1 - dice_score).mean()\n","\n","    def combined_loss(self, inputs, targets, alpha=0.5, beta=0.5, gamma=2.0, smooth=1e-6):\n","        loss_focal = self.focal_loss(inputs, targets, alpha=alpha, gamma=gamma)\n","        loss_dice = self.dice_loss(inputs, targets, smooth=smooth)\n","        return beta * loss_focal + (1 - beta) * loss_dice\n","\n","\n","    def train_step(self, inputs, target_mask, lossfunc):\n","        inputs, target_mask = inputs.to(self.device), target_mask.to(self.device)\n","        self.train()\n","        self.optimizer.zero_grad()\n","        pred_mask = self(inputs)\n","        mask_loss = lossfunc(pred_mask, target_mask)\n","\n","        mask_loss.backward()\n","        self.optimizer.step()\n","        return mask_loss.item()\n","\n","    def validate(self, val_loader, lossfunc):\n","        self.eval()\n","        total_loss = 0\n","        with torch.no_grad():\n","            for inputs, target_mask in val_loader:\n","                target_mask = target_mask.long()\n","                inputs, target_mask = inputs.to(device), target_mask.to(device)\n","                pred_mask = self(inputs)\n","                mask_loss = lossfunc(pred_mask, target_mask)\n","                total_loss += mask_loss.item()\n","        return total_loss / len(val_loader)\n","\n","    def train_model(self, full_dataset, epochs, batch_size, val_percent, loss='dice_loss'):\n","        alloss = {'dice_loss': self.dice_loss, 'focal_loss': self.focal_loss,\n","                  'combined_loss': self.combined_loss, 'ce_loss': self.ce_loss}\n","        lossfunc = alloss[loss]\n","\n","        for epoch in range(epochs):\n","            start_time = time.time()\n","\n","            n_val = int(len(full_dataset) * val_percent)\n","            n_train = len(full_dataset) - n_val\n","            train_subset, val_subset = torch.utils.data.random_split(full_dataset, [n_train, n_val])\n","\n","            train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n","            val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size)\n","\n","            total_train_loss = 0\n","            for inputs, target_mask in train_loader:\n","                target_mask = target_mask.long()\n","                mask_loss = self.train_step(inputs, target_mask, lossfunc)\n","                total_train_loss += mask_loss\n","            avg_train_loss = total_train_loss / len(train_loader)\n","            self.history[\"train_loss\"].append(avg_train_loss)\n","\n","            end_time = time.time()\n","            elapsed_time = end_time - start_time\n","\n","            if val_loader:\n","                avg_val_loss = self.validate(val_loader, lossfunc)\n","                self.history[\"val_loss\"].append(avg_val_loss)\n","            else:\n","                avg_val_loss = 'N/A'\n","\n","            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss} Time: {elapsed_time:.2f} sec\")\n","\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","    def predict(self, x):\n","        x = x.to(self.device)\n","        self.eval()\n","        with torch.no_grad():\n","            preds = self(x)\n","            preds = preds.argmax(dim=1)  #  (batch_size, height, width)\n","\n","            preds = preds.float() / (self.n_classes - 1) * 255\n","            preds = preds.to(torch.uint8)\n","\n","            return preds\n","\n","    def save_model(self, file_path):\n","        torch.save(self.state_dict(), file_path)\n","\n","    @staticmethod\n","    def load_model(file_path, n_channels, n_classes, bilinear=False):\n","        model = UNet(n_channels, n_classes, bilinear)\n","        model.load_state_dict(torch.load(file_path, map_location=model.device))\n","        return model\n","\n","class BasicDataset(Dataset):\n","    def __init__(self, images_dir, masks_dir, augmentation=False, mirror=True, rotate=True):\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.augmentation = augmentation\n","\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n","        ])\n","        self.transtensor = transforms.ToTensor()\n","\n","        self.images = [file for file in os.listdir(images_dir) if file.endswith('.png')]\n","        self.masks = [file for file in os.listdir(masks_dir) if file.endswith('.png')]\n","\n","        if self.augmentation:\n","            self.apply_augmentation(mirror, rotate)\n","\n","    def apply_augmentation(self, mirror, rotate):\n","        for img_name in self.images:\n","            img_path = os.path.join(self.images_dir, img_name)\n","            image = Image.open(img_path).convert(\"L\")\n","\n","            if mirror:\n","                mirrored_image = ImageOps.mirror(image)\n","                mirrored_image.save(os.path.join(self.images_dir, f'mirror_{img_name}'))\n","\n","            if rotate:\n","                rotated_image = image.rotate(180)\n","                rotated_image.save(os.path.join(self.images_dir, f'rotate_{img_name}'))\n","\n","        for mask_name in self.masks:\n","            mask_path = os.path.join(self.masks_dir, mask_name)\n","            mask = Image.open(mask_path).convert(\"L\")\n","\n","            if mirror:\n","                mirrored_mask = ImageOps.mirror(mask)\n","                mirrored_mask.save(os.path.join(self.masks_dir, f'mirror_{mask_name}'))\n","\n","            if rotate:\n","                rotated_mask = mask.rotate(180)\n","                rotated_mask.save(os.path.join(self.masks_dir, f'rotate_{mask_name}'))\n","\n","        # Update the lists of images and masks after augmentation\n","        self.images = [file for file in os.listdir(self.images_dir) if file.endswith('.png')]\n","        self.masks = [file for file in os.listdir(self.masks_dir) if file.endswith('.png')]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.images_dir, self.images[idx])\n","        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n","\n","        image = Image.open(img_path).convert(\"L\")\n","        mask = Image.open(mask_path).convert(\"L\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        else:\n","            image = self.transtensor(image)\n","\n","        mask = torch.from_numpy(np.array(mask))\n","\n","        return image, mask\n","\n","# Define the Zenodo link\n","zenodo_link = \"https://zenodo.org/records/10655519/files/starunet.zip?download=1\"\n","\n","# Download the file\n","response = requests.get(zenodo_link)\n","zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n","\n","# Specify the destination folder for extraction\n","destination_folder = '/content' + \"/model\"\n","\n","\n","# Extract the contents of the zip file to the destination folder\n","zip_file.extractall(destination_folder)\n","\n","# Close the zip file\n","zip_file.close()\n","\n","!rm -rf model/__MACOSX/\n","\n","zenodo_link = \"https://zenodo.org/records/10656286/files/small.tif?download=1\"\n","response = requests.get(zenodo_link)\n","\n","destination_path = '/content/demo.tif'\n","\n","with open(destination_path, 'wb') as file:\n","    file.write(response.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JFx0yY8lnpbw"},"outputs":[],"source":["# %%capture\n","\n","Source_image = \"/content/demo.tif\"  #@param{type:\"string\"}\n","\n","# preprocess\n","import tifffile as tiff\n","\n","def process_tiff(file_path):\n","    img = tiff.imread(file_path)\n","\n","    if img.ndim == 4:\n","        channel_data = img[:, :, :, 0]\n","\n","        if channel_data.dtype == np.uint16:\n","            channel_data = (channel_data / 256).astype(np.uint8)\n","\n","        processed_file_path = file_path.replace('.tif', '_processed.tif')\n","        tiff.imwrite(processed_file_path, channel_data)\n","\n","        return processed_file_path\n","\n","    elif img.ndim == 3:\n","        img = img[:, :, 0]\n","        if img.dtype == np.uint16:\n","            img = (img / 256).astype(np.uint8)\n","            processed_file_path = file_path.replace('.tif', '_processed.tif')\n","            tiff.imwrite(processed_file_path, img)\n","            return processed_file_path\n","        else:\n","            return file_path\n","    elif img.ndim == 2:\n","        if img.dtype == np.uint16:\n","            img = (img / 256).astype(np.uint8)\n","            return processed_file_path\n","        else:\n","            return file_path\n","\n","Source_image = process_tiff(Source_image)\n","\n","#split video\n","\n","if Source_image.split('.')[-1] == 'tif':\n","    img = tiff.TiffFile(Source_image).asarray()\n","else:\n","    img = cv2.imread(Source_image, cv2.IMREAD_UNCHANGED)\n","\n","save_folder = 'processing'\n","\n","if not os.path.exists(save_folder):\n","    os.makedirs(save_folder)\n","\n","if len(img.shape) == 3:\n","    for t in range(img.shape[0]):\n","        frame = img[t, :, :]\n","        frame_filename = os.path.join(save_folder, f'frame_{t}.png')\n","        cv2.imwrite(frame_filename, frame)\n","elif len(img.shape) == 2:\n","    frame = img.copy()\n","    frame_filename = os.path.join(save_folder, f\"{Source_image.split('/')[-1]}\")\n","    cv2.imwrite(frame_filename, frame)\n","\n","\n","# Stardist\n","Source_images_folder = '/content/processing'\n","close_processing_images = True #@param{type:\"boolean\"}\n","\n","def starpre(folder):\n","    base_path = '/content/model/starunet'\n","    model_name = 'star1'\n","    save_folder = os.path.join(base_path, model_name, \"Predictions\")\n","\n","    if not os.path.exists(save_folder):\n","        os.makedirs(save_folder)\n","\n","    Source_QC_folder_tif = os.path.join(folder, \"[!\\\\.]*\")\n","    Z = sorted(glob(Source_QC_folder_tif))\n","    file_paths = Z.copy()\n","    Z = list(map(imread, Z)) if Z[0].split('.')[-1] == 'tif' else [skimage.io.imread(str(file_path)) for file_path in Z]\n","    n_channel = 1 if Z[0].ndim == 2 else Z[0].shape[-1]\n","\n","    model = StarDist2D(None, name=model_name, basedir=base_path)\n","    axis_norm = (0, 1) if n_channel == 1 else (0, 1, 2)\n","    lbl_cmap = random_label_cmap()\n","\n","    loaded_model = tf.saved_model.load(os.path.join(base_path, model_name))\n","\n","    for i, img in tqdm(enumerate(Z), total=len(Z)):\n","        img_normalized = normalize(img, 1, 99.8, axis=axis_norm)\n","        labels, _ = model.predict_instances(img_normalized)\n","\n","        fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n","\n","        ax[0].imshow(img, cmap='gray')\n","        ax[0].set_title('Original Image')\n","        ax[0].axis('off')\n","\n","        ax[1].imshow(labels, cmap=lbl_cmap)\n","        ax[1].set_title('Predicted Mask')\n","        ax[1].axis('off')\n","\n","        ax[2].imshow(img, cmap='gray')\n","        ax[2].imshow(labels, cmap=lbl_cmap, alpha=0.5)  # ÈáçÂè†È¢ÑÊµãÊé©Á†ÅÔºåÈÄèÊòéÂ∫¶ËÆæÁΩÆ‰∏∫0.5\n","        ax[2].set_title('Overlap')\n","        ax[2].axis('off')\n","\n","        if close_processing_images:\n","          plt.close()\n","        # plt.show()\n","        # print(file_paths[i])\n","\n","        filename = os.path.basename(file_paths[i])\n","        imwrite(os.path.join(save_folder, filename), labels)\n","\n","starpre(Source_images_folder)\n","\n","def unetpre(image_dir, model):\n","    base_path = '/content/model/starunet'\n","    QC_model_name = 'unet1'\n","    save_folder = os.path.join(base_path, QC_model_name, \"Predictions\")\n","\n","    if not os.path.exists(save_folder):\n","        os.makedirs(save_folder)\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","    ]) #        transforms.Resize((256, 256)),\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    for filename in tqdm(os.listdir(image_dir)):\n","        if filename.endswith('.png'):\n","            image_path = os.path.join(image_dir, filename)\n","            image = Image.open(image_path).convert('L')\n","        elif filename.endswith('.tif'):\n","            image_path = os.path.join(image_dir, filename)\n","            image = skimage.io.imread(image_path)\n","\n","        input_image = transform(image).unsqueeze(0).to(device)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            prediction = model(input_image)\n","            prediction = prediction.argmax(dim=1).squeeze(0).cpu().numpy()\n","\n","        prediction_to_save = prediction.astype(np.uint8)\n","        imwrite(os.path.join(save_folder, filename), prediction_to_save)\n","\n","        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        axes[0].imshow(np.array(image), cmap='gray')\n","        axes[0].set_title('Original Image')\n","        axes[0].axis('off')\n","\n","        axes[1].imshow(prediction, cmap='CMRmap_r')\n","        axes[1].set_title('Predicted Image')\n","        axes[1].axis('off')\n","\n","        axes[2].imshow(np.array(image), cmap='gray', alpha=1)\n","        axes[2].imshow(prediction, cmap='CMRmap_r', alpha=0.5)\n","        axes[2].set_title('Overlap')\n","        axes[2].axis('off')\n","\n","        if close_processing_images:\n","            plt.close()\n","        #plt.show()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","unetmodel = UNet.load_model(\"/content/model/starunet/unet1/unet.pth\", n_channels=1, n_classes=5)\n","unetmodel = unetmodel.to(device)\n","unetpre(Source_images_folder, model = unetmodel)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46LycE5jMGv3","cellView":"form"},"outputs":[],"source":["close_processing_data = True #@param{type:\"boolean\"}\n","\n","import re\n","def sort_key(name):\n","    numbers = re.findall(r'\\d+', name)\n","    return int(numbers[0]) if numbers else 0\n","\n","def count_dir(dir_path, save_dir, ori_save_dir, line_thickness=3):\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","    if not os.path.exists(ori_save_dir):\n","        os.makedirs(ori_save_dir)\n","\n","    color_map = {'above': 'red', 'tem': 'blue', 'under': 'green'}\n","    color_map2 = {1: (255, 0, 0), 2: (0, 0, 255), 3: (0, 255, 0)}\n","    legend_elements = [Patch(facecolor=color, edgecolor=color, label=f'Label {label}') for label, color in color_map.items()]\n","\n","    counts = {'counts_above':[], 'counts_tem':[], 'counts_under':[]}\n","    map = {1:'count_above', 2:'count_tem', 3:'count_under'}\n","\n","    sorted_filenames = sorted(os.listdir(dir_path), key=sort_key)\n","\n","    for filename in tqdm(sorted_filenames):\n","        count = {'count_above':0, 'count_tem':0, 'count_under':0} # above label is 1, tem label is 2, under label is 3\n","        image_path = os.path.join(dir_path, filename)\n","\n","        image_star = np.array(Image.open(f'/content/model/starunet/star1/Predictions/{filename}'))\n","        image_unet = np.array(Image.open(f'/content/model/starunet/unet1/Predictions/{filename}'))\n","        original_image = np.array(Image.open(image_path).convert('RGB'))\n","        #print(f'orignal cells: {max(np.unique(image_star))}')\n","\n","        original_image_with_ori_boundary = np.array(Image.open(image_path).convert('RGB'))\n","        '''\n","        for i in range(1, np.max(image_star) + 1):\n","            mask_ori = (image_star == i)\n","            boundary_ori = ndimage.binary_dilation(mask_ori) ^ mask_ori\n","            original_image_with_ori_boundary[boundary_ori] = [255, 255, 255]  # ÁôΩËâ≤ËæπÁïå\n","\n","        plt.figure()\n","        plt.imshow(original_image_with_ori_boundary)\n","        plt.axis('off')\n","        plt.savefig(os.path.join(ori_save_dir, filename), bbox_inches='tight')\n","        plt.close()\n","        '''\n","        for index, cells in enumerate(np.unique(image_star)):\n","\n","            mask = (image_star == index)\n","            labels_in_region = image_unet[mask]\n","            label_counts = Counter(labels_in_region)\n","            top_three = label_counts.most_common(5)\n","\n","            '''\n","            total_count = sum(label_counts.values())\n","            print(f\"Region {i} in {filename}:\")\n","            for label, count in top_three:\n","                percentage = (count / total_count) * 100\n","                print(f\"  Label {label}: {count} ({percentage:.2f}%)\")\n","            '''\n","\n","            for label, _ in top_three:\n","                if label not in [0, 4]:\n","\n","                    count[map[label]] += 1\n","                    boundary = ndimage.binary_dilation(mask) ^ mask\n","                    color = color_map2.get(label, (255, 255, 255))\n","                    original_image[boundary] = cv2.cvtColor(np.uint8([[[color]]]), cv2.COLOR_RGB2BGR)[0][0]\n","                    break\n","\n","        if not close_processing_data:\n","            print(f'{filename}:{count}')\n","        #print(f'after cells: {sum(count.values())}')\n","\n","\n","        fig, ax = plt.subplots()\n","        ax.imshow(original_image)\n","        ax.legend(handles=legend_elements, loc='lower left')\n","        ax.axis('off')\n","        count_text = \"\\n\".join([f'Label {label}: {count}' for label, count in count.items()])\n","        plt.text(0.95, 0.05, count_text, verticalalignment='bottom', horizontalalignment='right', transform=ax.transAxes, color='white', fontsize=12)\n","\n","        # Save image\n","        plt.savefig(os.path.join(save_dir, filename), bbox_inches='tight')\n","        plt.close()\n","\n","        counts['counts_above'].append(count['count_above'])\n","        counts['counts_tem'].append(count['count_tem'])\n","        counts['counts_under'].append(count['count_under'])\n","\n","    df = pd.DataFrame(counts)\n","    df.index.name = 'time frame'\n","\n","    csv_filename = 'counts.csv'\n","    df.to_csv(csv_filename)\n","\n","    print(f'counts.csv data saved')\n","\n","    if not close_processing_data:\n","        print(counts)\n","\n","    return counts\n","\n","counts = count_dir(Source_images_folder, '/content/results/images', '/content/results/oris')\n","\n","\n","df = pd.read_csv('counts.csv', index_col='time frame')\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(df['counts_above'], label='Counts Above', marker='o')\n","plt.plot(df['counts_tem'], label='Counts Tem', marker='s')\n","plt.plot(df['counts_under'], label='Counts Under', marker='^')\n","\n","plt.title('Counts Over Time Frames')\n","plt.xlabel('Time Frame')\n","plt.ylabel('Counts')\n","plt.xticks(range(len(df)))\n","plt.legend()\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyON93XElbV6mX3xlcAPcm5a"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}